{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fa07a4f",
   "metadata": {},
   "source": [
    "# MLE challenge - Train model notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c09cfde",
   "metadata": {},
   "source": [
    "### Notebook 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c189b6",
   "metadata": {},
   "source": [
    "In this notebook, we train the model with a few features (for reasons of time and complexity in solving the challenge). It also shows how to persist the model in a file, load it into memory and then make a predict.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd0ee381",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.2.0-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/11/18 08:01:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "/usr/local/spark/python/pyspark/sql/context.py:77: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "sc = pyspark.SparkContext(master=\"local[1]\",appName=\"CreditRisk_N2_czavalaj\")\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bc180c",
   "metadata": {},
   "source": [
    "#### Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c98c1b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- years_on_the_job: integer (nullable = true)\n",
      " |-- nb_previous_loans: integer (nullable = true)\n",
      " |-- avg_amount_loans_previous: double (nullable = false)\n",
      " |-- flag_own_car: integer (nullable = true)\n",
      " |-- status: integer (nullable = true)\n",
      "\n",
      "+-------+---+----------------+-----------------+-------------------------+------------+------+\n",
      "|     id|age|years_on_the_job|nb_previous_loans|avg_amount_loans_previous|flag_own_car|status|\n",
      "+-------+---+----------------+-----------------+-------------------------+------------+------+\n",
      "|5008804| 33|              12|                0|                      0.0|           1|     0|\n",
      "|5008804| 33|              12|                1|       102.28336090329137|           1|     0|\n",
      "|5008804| 33|              12|                2|       119.44270503521452|           1|     0|\n",
      "|5008804| 33|              12|                3|        117.8730346375606|           1|     0|\n",
      "|5008804| 33|              12|                4|       114.28953848655442|           1|     0|\n",
      "|5008804| 33|              12|                5|       114.02126012338815|           1|     0|\n",
      "|5008804| 33|              12|                6|       116.29139797883522|           1|     0|\n",
      "|5008804| 33|              12|                7|       114.67762640447356|           1|     0|\n",
      "|5008804| 33|              12|                8|       122.59493127927432|           1|     0|\n",
      "|5008804| 33|              12|                9|       127.88591513832655|           1|     0|\n",
      "|5008804| 33|              12|               10|       126.30989214235919|           1|     0|\n",
      "|5008804| 33|              12|               11|       127.09100349039277|           1|     0|\n",
      "|5008804| 33|              12|               12|       133.61051710521372|           1|     0|\n",
      "|5008804| 33|              12|               13|       131.99517467789153|           1|     1|\n",
      "|5008804| 33|              12|               14|        130.1499230161449|           1|     0|\n",
      "|5008804| 33|              12|               15|       128.90637116931464|           1|     0|\n",
      "|5008805| 33|              12|                0|                      0.0|           1|     0|\n",
      "|5008805| 33|              12|                1|       114.67783357765765|           1|     0|\n",
      "|5008805| 33|              12|                2|       111.53915853106223|           1|     0|\n",
      "|5008805| 33|              12|                3|       109.69501256367808|           1|     0|\n",
      "+-------+---+----------------+-----------------+-------------------------+------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#cRiskDF = SparkContext.read.csv('/home/jovyan/train_model/part-00000-4168813d-3d69-47ae-b5e0-9b7fc41d2016-c000.csv',header=True)\n",
    "cRiskDF= sqlContext.read.csv('/home/jovyan/train_model/*.csv', sep=',', header='true', inferSchema='true')\n",
    "cRiskDF = cRiskDF.na.fill(value=0,subset=['avg_amount_loans_previous'])\n",
    "cRiskDF.printSchema()\n",
    "cRiskDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afaaa964",
   "metadata": {},
   "source": [
    "Let's begin with the estimators and the pipeline after that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf7edd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "numericCols = (['age', 'years_on_the_job', 'nb_previous_loans', 'avg_amount_loans_previous','flag_own_car'])\n",
    "# stage 1\n",
    "vectorizer = VectorAssembler(inputCols=numericCols, outputCol = 'features')\n",
    "#cRiskDF = vectorizer.transform(cRiskDF)\n",
    "#cRiskDF.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e24b5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "# stage 2\n",
    "label_stringIdx = StringIndexer(inputCol='status',outputCol='labelIndex')\n",
    "#cRiskDF = label_stringIdx.fit(cRiskDF).transform(cRiskDF)\n",
    "#cRiskDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430237d3",
   "metadata": {},
   "source": [
    "Let's split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df9f0641",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "543980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "233735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train_cRiskDF, test_cRiskDF = cRiskDF.randomSplit([0.7,0.3],seed=123)\n",
    "print(train_cRiskDF.cache().count())\n",
    "print(test_cRiskDF.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa51909",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59ba0353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ************ RANDOM FOREST ********************\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c9088f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a RandomForest model.\n",
    "rfc = RandomForestClassifier(numTrees=20, maxDepth=5, labelCol='labelIndex', featuresCol='features',predictionCol='prediction')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "93fc38c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain indexer and forest in a Pipeline\n",
    "pipeline = Pipeline(stages=[vectorizer,label_stringIdx,rfc])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0e33f1f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Train model. This also runs the indexer.\n",
    "model = pipeline.fit(train_cRiskDF)\n",
    "#model = rfc.fit(train_cRiskDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a1e6893a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 33:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+-----------------+-------------------------+------------+--------------------+----------+----------+\n",
      "|age|years_on_the_job|nb_previous_loans|avg_amount_loans_previous|flag_own_car|            features|labelIndex|prediction|\n",
      "+---+----------------+-----------------+-------------------------+------------+--------------------+----------+----------+\n",
      "| 33|              12|                2|       119.44270503521452|           1|[33.0,12.0,2.0,11...|       0.0|       0.0|\n",
      "| 33|              12|                6|       116.29139797883522|           1|[33.0,12.0,6.0,11...|       0.0|       0.0|\n",
      "| 33|              12|                9|       127.88591513832655|           1|[33.0,12.0,9.0,12...|       0.0|       0.0|\n",
      "| 33|              12|               12|       133.61051710521372|           1|[33.0,12.0,12.0,1...|       0.0|       0.0|\n",
      "| 33|              12|               13|       131.99517467789153|           1|[33.0,12.0,13.0,1...|       1.0|       0.0|\n",
      "+---+----------------+-----------------+-------------------------+------------+--------------------+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Make predictions.\n",
    "predictions = model.transform(test_cRiskDF)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select('age', 'years_on_the_job','nb_previous_loans','avg_amount_loans_previous','flag_own_car','features','labelIndex', 'prediction').show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9564d6a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 34:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.0149357\n",
      "RandomForestClassificationModel: uid=RandomForestClassifier_6aa661ec2b84, numTrees=20, numClasses=2, numFeatures=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"labelIndex\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))\n",
    "\n",
    "rfModel = model.stages[2]\n",
    "print(rfModel)  # summary only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "927c3904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9850642821999273\n"
     ]
    }
   ],
   "source": [
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073a3147",
   "metadata": {},
   "source": [
    "## Model persistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388669e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67756b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump model\n",
    "dump(model, 'model_risk.joblib') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aad1f36",
   "metadata": {},
   "source": [
    "### Load model & predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5296e542",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = load('model_risk.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fa7b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example dict 'user_id' -> features\n",
    "d = {\n",
    "    '5008804': [32, 12, 2, 119.45, 1],\n",
    "    '5008807': [29, 2, 1, 100, 0]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad3fd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model.predict([d['5008804']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ac04ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af2ff48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
